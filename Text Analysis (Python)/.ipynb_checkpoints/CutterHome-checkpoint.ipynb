{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /nfshome/ag4215/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import isbnlib\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from html.parser import HTMLParser\n",
    "import urllib\n",
    "from urllib import urlopen\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# record linkage package\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean, phonenumbers, phonetic\n",
    "\n",
    "from __future__ import print_function\n",
    "from six.moves import zip, range \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://thewirecutter.com/'\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "data = soup.findAll(text=True)\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "result = filter(visible, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_PREPROCESS = re.compile( r'\\W+|\\d+' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = np.array( [ re.sub( RE_PREPROCESS, ' ', description ).lower() for description in result ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words( corpus,\n",
    "                         NGRAM_RANGE = ( 0, 1 ),\n",
    "                         stop_words = None,\n",
    "                         stem = False,\n",
    "                         MIN_DF = 0.005,\n",
    "                         MAX_DF = 0.95,\n",
    "                         USE_IDF = False ):\n",
    "\n",
    "    \"\"\"\n",
    "    Turn a corpus of text into a bag-of-words.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: ls\n",
    "        test of documents in corpus    \n",
    "    NGRAM_RANGE: tuple\n",
    "        range of N-gram. Default (0,1)\n",
    "    stop_words: ls\n",
    "        list of commonly occuring words that have little semantic\n",
    "        value\n",
    "    stem: bool\n",
    "        use a stemmer to stem words\n",
    "    MIN_DF: float\n",
    "       exclude words that have a frequency less than the threshold\n",
    "    MAX_DF: float\n",
    "        exclude words that have a frequency greater than the threshold\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bag_of_words: scipy sparse matrix\n",
    "        scipy sparse matrix of text\n",
    "    features:\n",
    "        ls of words\n",
    "    \"\"\"\n",
    "    #parameters for vectorizer \n",
    "    ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "    STRIP_ACCENTS = 'unicode'\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "    if stem:\n",
    "        tokenize = lambda x: [stemmer.stem(i) for i in x.split()]\n",
    "    else:\n",
    "        tokenize = None\n",
    "    vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                                tokenizer=tokenize, \n",
    "                                ngram_range=NGRAM_RANGE,\n",
    "                                stop_words = stop_words,\n",
    "                                strip_accents=STRIP_ACCENTS,\n",
    "                                min_df = MIN_DF,\n",
    "                                max_df = MAX_DF)\n",
    "    \n",
    "    bag_of_words = vectorizer.fit_transform( corpus ) #transform our corpus is a bag of words \n",
    "    features = vectorizer.get_feature_names()\n",
    "\n",
    "    if USE_IDF:\n",
    "        NORM = None #turn on normalization flag\n",
    "        SMOOTH_IDF = True #prvents division by zero errors\n",
    "        SUBLINEAR_IDF = True #replace TF with 1 + log(TF)\n",
    "        transformer = TfidfTransformer(norm = NORM,smooth_idf = SMOOTH_IDF,sublinear_tf = True)\n",
    "        #get the bag-of-words from the vectorizer and\n",
    "        #then use TFIDF to limit the tokens found throughout the text \n",
    "        tfidf = transformer.fit_transform(bag_of_words)\n",
    "        \n",
    "        return tfidf, features\n",
    "    else:\n",
    "        return bag_of_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a bag of words with PROCESSED corpus\n",
    "bag_of_words, features = create_bag_of_words(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'accessories',\n",
       " u'after',\n",
       " u'all',\n",
       " u'and',\n",
       " u'appliances',\n",
       " u'baby',\n",
       " u'best',\n",
       " u'by',\n",
       " u'cars',\n",
       " u'for',\n",
       " u'gear',\n",
       " u'home',\n",
       " u'in',\n",
       " u'is',\n",
       " u'may',\n",
       " u'more',\n",
       " u'of',\n",
       " u'office',\n",
       " u'or',\n",
       " u'see',\n",
       " u'testing',\n",
       " u'that',\n",
       " u'the',\n",
       " u'to',\n",
       " u'travel',\n",
       " u'updated',\n",
       " u'we']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts( bag_of_words, feature_names ):\n",
    "\n",
    "    \"\"\"\n",
    "    Get the ordered word counts from a bag_of_words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bag_of_words: obj\n",
    "        scipy sparse matrix from CounterVectorizer\n",
    "    feature_names: ls\n",
    "        list of words\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    word_counts: dict\n",
    "        Dictionary of word counts\n",
    "    \"\"\"\n",
    "\n",
    "    # convert bag of words to array\n",
    "    np_bag_of_words = bag_of_words.toarray()\n",
    "    \n",
    "    # calculate word count.\n",
    "    word_count = np.sum(np_bag_of_words,axis=0)\n",
    "    \n",
    "    # convert to flattened array.\n",
    "    np_word_count = np.asarray(word_count).ravel()\n",
    "    \n",
    "    # create dict of words mapped to count of occurrences of each word.\n",
    "    dict_word_counts = dict( zip(feature_names, np_word_count) )\n",
    "    \n",
    "    # Create ordered dictionary\n",
    "    orddict_word_counts = OrderedDict( sorted(dict_word_counts.items(), key=lambda x: x[1], reverse=True), )\n",
    "    \n",
    "    return orddict_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(u'the', 127),\n",
       "             (u'best', 73),\n",
       "             (u'and', 62),\n",
       "             (u'all', 55),\n",
       "             (u'we', 42),\n",
       "             (u'for', 41),\n",
       "             (u'of', 30),\n",
       "             (u'updated', 29),\n",
       "             (u'to', 28),\n",
       "             (u'home', 27),\n",
       "             (u'by', 26),\n",
       "             (u'may', 24),\n",
       "             (u'appliances', 23),\n",
       "             (u'is', 22),\n",
       "             (u'after', 21),\n",
       "             (u'in', 20),\n",
       "             (u'more', 18),\n",
       "             (u'that', 18),\n",
       "             (u'office', 17),\n",
       "             (u'accessories', 16),\n",
       "             (u'gear', 16),\n",
       "             (u'baby', 16),\n",
       "             (u'cars', 15),\n",
       "             (u'testing', 14),\n",
       "             (u'see', 14),\n",
       "             (u'travel', 14),\n",
       "             (u'or', 14)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_counts(bag_of_words, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(u'best', 73),\n",
       "             (u'updated', 29),\n",
       "             (u'home', 27),\n",
       "             (u'may', 24),\n",
       "             (u'appliances', 23),\n",
       "             (u'office', 17),\n",
       "             (u'gear', 16),\n",
       "             (u'accessories', 16),\n",
       "             (u'baby', 16),\n",
       "             (u'cars', 15),\n",
       "             (u'travel', 14),\n",
       "             (u'testing', 14),\n",
       "             (u'see', 14)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_bag_of_words, processed_features = create_bag_of_words(processed_corpus, stop_words = eng_stopwords)\n",
    "dict_processed_word_counts = get_word_counts(processed_bag_of_words, processed_features)\n",
    "dict_processed_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dict_processed_word_counts, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.rename('Words', inplace=True)\n",
    "df.columns = ['Frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Words</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>updated</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appliances</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>office</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gear</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accessories</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baby</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cars</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>testing</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Frequency\n",
       "Words                 \n",
       "best                73\n",
       "updated             29\n",
       "home                27\n",
       "may                 24\n",
       "appliances          23\n",
       "office              17\n",
       "gear                16\n",
       "accessories         16\n",
       "baby                16\n",
       "cars                15\n",
       "travel              14\n",
       "testing             14\n",
       "see                 14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibStats",
   "language": "python",
   "name": "libstats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
